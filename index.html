<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chengyi Xing</title>

    <meta name="author" content="Chengyi Xing">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <link rel="apple-touch-icon" sizes="57x57" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-57x57.png" />
<link rel="apple-touch-icon" sizes="60x60" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-60x60.png" />
<link rel="apple-touch-icon" sizes="72x72" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon" sizes="76x76" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon" sizes="114x114" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-114x114.png" />
<link rel="apple-touch-icon" sizes="120x120" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon" sizes="144x144" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-144x144.png" />
<link rel="apple-touch-icon" sizes="152x152" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-152x152.png" />
<link rel="apple-touch-icon" sizes="180x180" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-180x180.png" />

<link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-196x196.png" sizes="196x196" />
<link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-192x192.png" sizes="192x192" />
<link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-128.png" sizes="128x128" />
<link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-16x16.png" sizes="16x16" />

<link rel="mask-icon" href="//www-media.stanford.edu/assets/favicon/safari-pinned-tab.svg" color="#ffffff">
<meta name="application-name" content="Stanford University"/>
<meta name="msapplication-TileColor" content="#FFFFFF" />
<meta name="msapplication-TileImage" content="//www-media.stanford.edu/assets/favicon/mstile-144x144.png" />
<meta name="msapplication-square70x70logo" content="//www-media.stanford.edu/assets/favicon/mstile-70x70.png" />
<meta name="msapplication-square150x150logo" content="//www-media.stanford.edu/assets/favicon/mstile-150x150.png" />
<meta name="msapplication-square310x310logo" content="//www-media.stanford.edu/assets/favicon/mstile-310x310.png" />

  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chengyi Xing
                </p>
                <p> I am a Master's student in
                   <a href="https://ccrma.stanford.edu/" target="_blank"> CCRMA </a> at 
                   <a href="https://www.stanford.edu/" target="_blank">Stanford University</a>. 
                   I am currently doing research in the
                   <a href="http://bdml.stanford.edu/Main/HomePage" target="_blank">Biomimetics & Dexterous Manipulation Laboratory</a>,
                  advised by <a href="http://bdml.stanford.edu/Profiles/MarkCutkosky" target="_blank">Prof. Mark Cutkosky</a> and <a href="https://music.stanford.edu/people/elizabeth-schumann" target="_blank">Prof. Elizabeth Schumann</a>. 
                  Broadly, my research interest lies in designing robotic hardware that are:
                  <br> 
                  <em>data-collection-efficient, easy-to-fabricate, and low-cost. </em>
                </p>
                <p>
                  Currently, I am working on tactile sensors, focusing on wearable tactile sensors and whisker sensors 
                  for the underwater robot <a href="https://khatib.stanford.edu/ocean-one-k.html">OceanOneK</a>. 
                  Previously, I had the privilege of being advised by 
                  <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Prof. Wei-Shi Zheng</a> at 
                  Sun Yat-Sen University, where I worked on dexterous hand manipulation.
                </p>
                <!-- <p>
                  Besides robotics, I am also working on interdisciplinary research in music, healthcare, hardware, and AI. 
                  I am fortunate to collaborate with professors from the Music and Medical Schools.
                </p> -->
                <!-- <div align="center"><p>Email: chengyix at stanford dot edu</p></div> -->
                <p style="text-align:center">
                  <span>Email: chengyix at stanford dot edu</span> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=BglGZXEAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/xcyhbp/">LinkedIn</a> &nbsp;&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/chengyi.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding-left:20px;width:50%;vertical-align:top">
                <h2>Research</h2>
                <p>
                  (*equal contribution) 
                  </p>
              </td>
      </tr>
      <tr>
          <td style="padding:20px;width:45%;vertical-align:top">
            <a href="https://sites.google.com/stanford.edu/TacCap"> <img style="width:107%;max-width:107%" src="images/TacCap.png" class="hoverZoomLink"></a>
          </td>
          <td style="padding:20px;width:70%;vertical-align:top">
            <a href="https://sites.google.com/stanford.edu/TacCap">
            <!-- heading -->
            <papertitle><strong>TacCap: A Wearable FBG-Based Tactile Sensor for Seamless Human-to-Robot Skill Transfer</strong></papertitle>
            </a>
            <!-- authors -->
            <br>
            <strong>Chengyi Xing*</strong>,
            <a href="https://hao-l1.github.io/">Hao Li*</a>,
            <a href="https://wyl2077.github.io/">Yi-Lin Wei</a>,
            <a href="https://www.linkedin.com/in/teoren/">Tian-Ao Ren</a>,
            <a href="https://www.linkedin.com/in/tianyu-tu-b9b83a291/">Tianyu Tu</a>,
            Yuhao Lin,
            <a href=https://music.stanford.edu/people/elizabeth-schumann>Elizabeth Schumann</a>,
            <a href=https://www.isee-ai.cn/~zhwshi>Wei-Shi Zheng</a>,
            <a href=http://bdml.stanford.edu/Profiles/MarkCutkosky>Mark Cutkosky</a>
            <!-- conference & date -->
            <br>
            <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025</em>
            <br>
            <!-- links -->
            <a href="https://arxiv.org/pdf/2503.01789">PDF</a>
            / <a href="https://sites.google.com/stanford.edu/TacCap">Project Page</a>
            / <a href="https://arxiv.org/abs/2503.01789">arXiv</a>
            <!-- / <a href="TBD">code</a> -->
            <p></p>
            <p>TL;DR: Fingertip-wearable tactile sensor for data collection without a robot. Minimizes domain gap via human-robot sensor swap. Durable, easy-to-fabricate (~20 minutes per finger), 2kHz sampling rate, and EMI-resistant.</p>
          </td>
        </tr>

      <td style="padding:20px;width:50%;vertical-align:top">
        <a href="https://sites.google.com/stanford.edu/whiskertactile/home">
          <video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
          <source src="data/whisker_ral.mp4" type="video/mp4" />
        </video></a>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/stanford.edu/whiskertactile/home">
			<span class="papertitle">Whisker-Inspired Tactile Sensing: A Sim2Real Approach for Precise Underwater Contact Tracking
      </span>
        </a>
        <br>
				<a href="https://cs.stanford.edu/~hao.li" target="_blank">Hao Li*</a>, 
      <strong>Chengyi Xing*</strong>,
				<a href="https://profiles.stanford.edu/saad-ahmed-khan" target="_blank">Saad Khan</a>,
        <a href="https://miaoyazhong.github.io/" target="_blank">Miaoya Zhong</a>,
        <a href="http://bdml.stanford.edu/Profiles/MarkCutkosky" target="_blank">Mark Cutkosky</a>
				<br>
        <em>Robotics and Automation Letters (<strong>RA-L</strong>)</em>
        <br><span style="padding-top: 6px; display: block;"></span>
        <a href="https://arxiv.org/pdf/2410.14005">PDF</a>
        /
        <a href="https://sites.google.com/stanford.edu/whiskertactile/home">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2410.14005">arXiv</a>
        / <a href="https://github.com/hao-l1/WhiskerFBG">Code</a>
        <p></p>
        <p>
				TL;DR: Localize contacts with a casual transformer trained on simulation data in MuJoCo. 
        Designed with optic fiber, the whisker sensors are suitable for the underwater robot OceanOneK and are resistant to salt erosion.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:50%;vertical-align:top">
        <a href="https://sites.google.com/view/ijrr-2024-whisker/home">
          <video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
          <source src="data/whisker_ijrr.mp4" type="video/mp4" />
        </video></a>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/ijrr-2024-whisker/home">
			<span class="papertitle">Navigation and 3D Surface Reconstruction from Passive Whisker Sensing
</span>
        </a>
        <br>
				<a href="https://www.michaelalin.com/" target="_blank">Michael A. Lin</a>, 
				<a href="https://cs.stanford.edu/~hao.li" target="_blank">Hao Li</a>,
      <strong>Chengyi Xing</strong>,
        <a href="http://bdml.stanford.edu/Profiles/MarkCutkosky" target="_blank">Mark Cutkosky</a>
				<br>
        <em>International Journal of Robotics Research (IJRR), Under Review</em>
        <br><span style="padding-top: 6px; display: block;"></span>
        <a href="https://arxiv.org/pdf/2406.06038">PDF</a>
        /
        <a href="https://sites.google.com/view/ijrr-2024-whisker/home">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2406.06038">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=Rvx4tWSkfu8">Video</a>
        /
        <a href="https://github.com/HaoLiRobo/WhiskerSensing/tree/main?tab=readme-ov-file">Code</a>
        <p></p>
        <p>
				TL;DR: Non-intrusive contact localization using Hall effect sensors to prevent collisions.
        </p>
      </td>

    </tr>

    <tr>
      <td style="padding:20px;width:50%;vertical-align:top">
        <a href="https://sites.google.com/stanford.edu/dexgys">
          <video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
          <source src="data/grasp_as_you_say_neurips.mp4" type="video/mp4" />
        </video></a>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/stanford.edu/dexgys">
			<span class="papertitle">Grasp as You Say: Language-guided Dexterous Grasp Generation
</span>
        </a>
        <br>
				<a href="https://wyl2077.github.io/" target="_blank">Yi-Lin Wei</a>,
        <a href="https://jianjian-jiang.github.io/" target="_blank">Jian-Jian Jiang</a>, 
        <strong>Chengyi Xing</strong>,
        <span>Xiantuo Tan</span>, 
				<a href="https://dravenalg.github.io/" target="_blank">Xiao-Ming Wu</a>, 
        <a href="https://cs.stanford.edu/~hao.li" target="_blank">Hao Li</a>,
				<a href="http://bdml.stanford.edu/Profiles/MarkCutkosky" target="_blank">Mark Cutkosky</a>,
        <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>
        <br>
        <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024 
        <br><span style="padding-top: 6px; display: block;"></span>  
        <a href="https://arxiv.org/pdf/2405.19291">PDF</a>
        /
        <a href="https://sites.google.com/stanford.edu/dexgys">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2405.19291">arXiv</a>
        <p></p>
        <p>
				TL;DR: Synthesize a text-grasping dataset using GPT-4 from low-level hand-object features. 
        Generate dexterous hand grasps with a diffusion model conditioned on CLIP-decoded text embeddings.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:50%;vertical-align:top">
        <a href="https://arxiv.org/abs/2404.15815">
          <video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
          <source src="data/s2hg_cvpr.mp4" type="video/mp4" />
        </video></a>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.15815">
          <span class="papertitle">Single-View Scene Point Cloud Human Grasp Generation</span>
        </a>
        <br>
				
        <span>Yan-Kang Wang</span>,
        <strong>Chengyi Xing</strong>,
				<a href="https://wyl2077.github.io/" target="_blank">Yi-Lin Wei</a>,
				<a href="https://dravenalg.github.io/" target="_blank">Xiao-Ming Wu</a>, 
				<a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>
        <br>
        <em>Computer Vision and Pattern Recognition Conference (CVPR) </em>, 2024
        <br><span style="padding-top: 6px; display: block;"></span>
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.pdf">PDF</a>
        /
        <a href="https://arxiv.org/abs/2404.15815">arXiv</a>
        <p></p>
        <p>
        TL;DR: Generate hand grasps on single-view scene point clouds, preventing penetration into unseen parts.
        </p>
      </td>
    </tr>



    <tr>
      <td style="padding:20px;width:50%;vertical-align:top">
        <a href="https://ai-muzic.github.io/musecoco/">
            <img src="images/musecoco.png" alt="Musecoco" style="width:107%;max-width:107%" />
        </a>
    </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ai-muzic.github.io/musecoco/">
          <span class="papertitle">Musecoco: Generating Symbolic Music from Text</span>
        </a>
        <br>
				
        <a href="https://scholar.google.com/citations?user=tmHbfcEAAAAJ&hl=en" target="_blank">Peiling Lu*</a>,
        <a href="https://xxupiano.github.io/" target="_blank">Xin Xu*</a>,
				<span>Chenfei Kang*</span>,
				<a href="https://btyu.github.io/" target="_blank">Botao Yu*</a>, 
				<strong>Chengyi Xing*</strong>,
				<a href="https://tan-xu.github.io/" target="_blank">Xu Tan</a>,
				<a href="https://sites.google.com/view/jiangbian" target="_blank">Jiang Bian</a>
        <br><span style="padding-top: 6px; display: block;"></span>
        <a href="https://arxiv.org/pdf/2306.00110">PDF</a>
        /
        <a href="https://ai-muzic.github.io/musecoco/">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2306.00110">arXiv</a>
        /
        <a href="https://github.com/microsoft/muzic/tree/main/musecoco">Code</a>
        <p></p>
        <p>
        TL;DR: Synthesize a text-to-music dataset using GPT from extracted musical attributes. 
        A two-stage text-to-music framework, BERT for text understanding and a transformer decoder for music generation.
        </p>
      </td>
    </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Misc</h2>
                <p> I love jazz, J-POP, and AI-generated music 
                  (check out <a href="https://aitestkitchen.withgoogle.com/tools/music-fx-dj">MusicFX DJ</a> !)
                  You can often find me either playing random music or working out at the gym. </p>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        </td>
      </tr>
    </table>
  </body>
</html>
